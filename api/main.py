from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os

# --- INITIALISATION ---
app = FastAPI(title="API de Traduction NMT (Hybrid)", version="2.0")

# On v√©rifie si on est sur Render gr√¢ce √† une variable d'environnement
IS_ON_RENDER = os.environ.get('RENDER', False)

# Variables globales pour stocker le mod√®le (si on peut le charger)
model = None
tokenizer = None
use_fallback = False # Si True, on utilise deep-translator

print(f"üñ•Ô∏è Environnement d√©tect√© : {'CLOUD (Render)' if IS_ON_RENDER else 'LOCAL'}")

# --- TENTATIVE DE CHARGEMENT DU MOD√àLE IA ---
if not IS_ON_RENDER:
    # On ne tente de charger l'IA que si on est en LOCAL (pour √©conomiser la RAM sur Render)
    try:
        from transformers import MarianMTModel, MarianTokenizer
        import torch
        
        print("‚è≥ Chargement du mod√®le MarianMT (Local)...")
        model_name = "./modele_final_local"
        if not os.path.exists(model_name):
            model_name = "Helsinki-NLP/opus-mt-en-fr"
            
        tokenizer = MarianTokenizer.from_pretrained(model_name)
        model = MarianMTModel.from_pretrained(model_name)
        print("‚úÖ Mod√®le IA charg√© avec succ√®s !")
        
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur chargement IA : {e}")
        print("üîÑ Bascule automatique vers le mode 'Fallback'")
        use_fallback = True
else:
    # Sur Render, on passe directement en mode l√©ger
    print("‚òÅÔ∏è Mode Cloud activ√© : Utilisation de deep-translator pour √©conomiser la RAM.")
    use_fallback = True

# --- IMPORT DU FALLBACK (Si n√©cessaire) ---
if use_fallback:
    try:
        from deep_translator import GoogleTranslator
        print("‚úÖ Module de traduction l√©ger pr√™t.")
    except ImportError:
        print("‚ùå Erreur critique : deep-translator manquant.")

# --- ROUTE API ---
class TranslationRequest(BaseModel):
    text: str

@app.post("/translate")
def translate(request: TranslationRequest):
    if not request.text:
        raise HTTPException(status_code=400, detail="Texte vide")
    
    try:
        if not use_fallback and model and tokenizer:
            # M√âTHODE 1 : TON MOD√àLE IA (Local)
            inputs = tokenizer(request.text, return_tensors="pt", padding=True, truncation=True)
            translated = model.generate(**inputs)
            resultat = [tokenizer.decode(t, skip_special_tokens=True) for t in translated][0]
            source = "Mod√®le MarianMT (IA Locale)"
        else:
            # M√âTHODE 2 : MODE L√âGER (Cloud / Render)
            resultat = GoogleTranslator(source='en', target='fr').translate(request.text)
            source = "Traducteur Cloud (Optimis√© RAM)"
            
        return {
            "original": request.text, 
            "traduction": resultat,
            "moteur": source
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/")
def home():
    return {"status": "online", "mode": "Cloud" if use_fallback else "Local AI"}